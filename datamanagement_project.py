# -*- coding: utf-8 -*-
"""DataManagement Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oLELk2HyYGDvJuEcOI5LtJB-wOv25Vda

# Importing Datasets
"""

import pandas as pd
import numpy as np
import re
import sqlite3  # Add this line
from scipy.spatial import cKDTree

import pandas as pd
data1 = pd.read_csv('/content/NY-House-Dataset.csv')
data2 = pd.read_csv('/content/NYPD_Arrests.csv', engine='python')

data1.head()

data2.head()

"""# NY House Dataset - Data Cleaning"""

clean1 = data1.copy()

# Check for missing values and duplicates ----
print(f"The amount of missing values in data1 is {data1.isna().sum().sum()}.")
print(f"The amount of duplicates in data1 is {data1.duplicated().sum()}.")

clean1 = clean1.drop_duplicates()
print(f"The amount of duplicates in clean1 is {clean1.duplicated().sum()}.")

# Standardize numeric data ----
#print(clean1.dtypes)
# PRICE, BEDS, BATH, PROPERTYSQFT, LATITUDE, LONGITUDE

clean1['BATH'] = clean1['BATH'].astype(int)
clean1['PROPERTYSQFT'] = clean1['PROPERTYSQFT'].astype(int)

# Handle outliers ----
#print(data1['BATH'].describe())
#print(data1['BEDS'].describe())
#print(data1['PRICE'].describe())
#print(data1['PROPERTYSQFT'].describe())

# Note that multi-family homes will naturally have high numbers of beds/baths. Manually remove unfeasible ratios of bed/baths.

bed_bath_subset = clean1[['BEDS','BATH']]
mu_bed_bath = bed_bath_subset.mean()
sigma_bed_bath = bed_bath_subset.std()
z_bed_bath = (bed_bath_subset - mu_bed_bath)/sigma_bed_bath

threshold = 4 # Detect only extreme outliers
outlier_mask = (abs(z_bed_bath > threshold)).any(axis=1)
outliers = clean1.loc[outlier_mask, ['BEDS','BATH']]
print(f"List of outlier values: {outliers}")

clean1.drop([4691,622], inplace=True)

# Drop unnecessary columns
clean1.drop(['BROKERTITLE','LONG_NAME','STREET_NAME','ADMINISTRATIVE_AREA_LEVEL_2','ADDRESS','MAIN_ADDRESS','FORMATTED_ADDRESS','LOCALITY'],axis=1,inplace=True)

# Standardize text labels ----
#clean1['ADMINISTRATIVE_AREA_LEVEL_2'].unique()
clean1['SUBLOCALITY'].unique() # Retrieve Borough
#clean1['STATE'].unique() # Retrieve ZIP code

borough_mapping = {
    'New York County':'Manhattan',
    'Kings County':'Brooklyn',
    'Queens County':'Queens',
    'Bronx County':'The Bronx',
    'Richmond County':'Staten Island',
    # Map neighborhood names to their boroughs
    'East Bronx':'The Bronx',
    'Riverdale':'The Bronx',
    'Coney Island':'Brooklyn',
    'Brooklyn Heights':'Brooklyn',
    'Fort Hamilton':'Brooklyn',
    'Dumbo':'Brooklyn',
    'Snyder Avenue':'Brooklyn',
    'Jackson Heights':'Queens',
    'Rego Park':'Queens',
    'Flushing':'Queens',
    'New York':'Manhattan' # New York = New York County = Manhattan
}
clean1['SUBLOCALITY'] = clean1['SUBLOCALITY'].replace(borough_mapping)
clean1.rename(columns={'SUBLOCALITY':'BOROUGH'},inplace=True)

# Regex to get the ZIP code in STATE
clean1['ZIP_CODE'] = clean1['STATE'].str.extract(r'(\d{5})')
clean1.drop(['STATE'],axis=1,inplace=True)

# Clean property type labels
clean1['TYPE'].unique()
clean1['TYPE'] = clean1['TYPE'].str.split(' ').str[0]

type_mapping = {
    'Multi-family':'Multi-family home',
    'For':'For sale',
    'Coming':'Coming soon',
    'Mobile':'Mobile house',
    'Condop':'Condo'
}
clean1['TYPE'] = clean1['TYPE'].replace(type_mapping)
clean1.head()

# Apply a unique property ID to each property in the cleaned dataset

clean1['PROPERTY_ID'] = range(1, len(clean1) + 1)

"""# NYPD Arrests Cleaning"""

# Drop the columns you do NOT need
cols_to_drop = [
    "PD_CD", "OFNS_DESC", "PD_DESC",
    "KY_CD", "JURISDICTION_CODE", "PERP_SEX"
]
data2 = data2.drop(columns=cols_to_drop)

# Create Borough column mapping
borough_map = {
    "B": "Bronx",
    "S": "Staten Island",
    "K": "Brooklyn",
    "M": "Manhattan",
    "Q": "Queens"
}

data2["BOROUGH"] = data2["ARREST_BORO"].map(borough_map)

# Reorder columns (optional but clean)
cols_order = [
    "ARREST_DATE", "LAW_CODE", "LAW_CAT_CD",
    "ARREST_BORO", "BOROUGH", "ARREST_PRECINCT",
    "AGE_GROUP", "PERP_RACE", "X_COORD_CD",
    "Y_COORD_CD", "Latitude", "Longitude"
]
data2 = data2[cols_order]

data2["ARREST_DATE"] = pd.to_datetime(data2["ARREST_DATE"])

data2 = data2.drop_duplicates()

data2.isna().sum()

data2["AGE_GROUP"] = data2["AGE_GROUP"].fillna("UNKNOWN")
data2["PERP_RACE"] = data2["PERP_RACE"].fillna("UNKNOWN")
data2 = data2.dropna(subset=["Latitude", "Longitude"])

data2["PERP_RACE"] = data2["PERP_RACE"].str.title().str.strip()
data2["AGE_GROUP"] = data2["AGE_GROUP"].str.replace(" ", "")

data2 = data2[
    (data2["Latitude"].between(40.48, 40.93)) &
    (data2["Longitude"].between(-74.28, -73.68))
]

law_map = {"F": "Felony", "M": "Misdemeanor", "V": "Violation"}
data2["LAW_CATEGORY"] = data2["LAW_CAT_CD"].map(law_map)

# Save cleaned dataset
data2.to_csv("clean_data2.csv", index=False)

data2.head()

clean1

"""# Database Schema

Properties(PROPERTY_ID)
*   Property Type
*   Price
*   Beds
*   Bath
*   Property SqFt
*   Borough
*   Latitude
*   Longitude
*   Zip code

Crimes(ARREST_KEY)
*   Law category
*   Borough
*   Arrest date
*   Latitude
*   Longitude
*   Arrest precint

Join tables based on aggregates within boroughs or by spatial joins.

# Spatial Joins
"""

!pip install gradio

import gradio as gr
import pandas as pd
import numpy as np
from math import radians, cos, sin, asin, sqrt
import re
import sqlite3  # Add this line
from scipy.spatial import cKDTree  # If you haven't imported this yet

# Haversine formula to calculate distance between two lat/lon points in miles
def haversine_distance(lat1, lon1, lat2, lon2):
    """
    Calculate the great circle distance between two points
    on the earth (specified in decimal degrees).
    Returns distance in miles.
    """
    # Convert decimal degrees to radians
    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])

    # Haversine formula
    dlat = lat2 - lat1
    dlon = lon2 - lon1
    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2
    c = 2 * asin(sqrt(a))

    # Radius of earth in miles
    r = 3956
    return c * r

def find_crimes_near_property(property_row, crime_df, radius_miles=0.5):
    """
    Find all crimes within a specified radius of a property.

    Parameters:
    - property_row: Single row from housing dataframe
    - crime_df: Crime dataframe
    - radius_miles: Search radius in miles (default 0.5 = ~8-10 blocks)

    Returns:
    - DataFrame of crimes within radius
    """
    prop_lat = property_row['LATITUDE']
    prop_lon = property_row['LONGITUDE']

    # Calculate distance for all crimes
    crime_df['distance_miles'] = crime_df.apply(
        lambda row: haversine_distance(
            prop_lat, prop_lon,
            row['Latitude'], row['Longitude']
        ),
        axis=1
    )

    # Filter crimes within radius
    nearby_crimes = crime_df[crime_df['distance_miles'] <= radius_miles].copy()

    return nearby_crimes

def aggregate_crime_features(property_row, crime_df, radius_miles=0.5):
    """
    Calculate crime statistics for a property's surrounding area.

    Returns a dictionary of crime features.
    """
    nearby_crimes = find_crimes_near_property(property_row, crime_df, radius_miles)

    features = {
        'PROPERTY_ID': property_row.get('PROPERTY_ID', 9999),
        'total_crimes': len(nearby_crimes),
        'crime_density': len(nearby_crimes) / (np.pi * radius_miles**2),
    }

    # Count by crime category
    if len(nearby_crimes) > 0:
        crime_counts = nearby_crimes['LAW_CATEGORY'].value_counts()
        features['felony_count'] = crime_counts.get('Felony', 0)
        features['misdemeanor_count'] = crime_counts.get('Misdemeanor', 0)
        features['violation_count'] = crime_counts.get('Violation', 0)

        # Calculate percentages
        features['felony_pct'] = (features['felony_count'] / features['total_crimes'] * 100) if features['total_crimes'] > 0 else 0
        features['misdemeanor_pct'] = (features['misdemeanor_count'] / features['total_crimes'] * 100) if features['total_crimes'] > 0 else 0
        features['violation_pct'] = (features['violation_count'] / features['total_crimes'] * 100) if features['total_crimes'] > 0 else 0

        # Temporal features: crimes in last 90 days
        if 'ARREST_DATE' in nearby_crimes.columns:
            recent_date = nearby_crimes['ARREST_DATE'].max()
            date_90_days_ago = recent_date - pd.Timedelta(days=90)
            recent_crimes = nearby_crimes[nearby_crimes['ARREST_DATE'] >= date_90_days_ago]
            features['crimes_last_90_days'] = len(recent_crimes)
        else:
            features['crimes_last_90_days'] = 0

        # Average distance to crimes
        features['avg_crime_distance'] = nearby_crimes['distance_miles'].mean()
        features['min_crime_distance'] = nearby_crimes['distance_miles'].min()
    else:
        # No crimes found
        features.update({
            'felony_count': 0,
            'misdemeanor_count': 0,
            'violation_count': 0,
            'felony_pct': 0,
            'misdemeanor_pct': 0,
            'violation_pct': 0,
            'crimes_last_90_days': 0,
            'avg_crime_distance': np.nan,
            'min_crime_distance': np.nan
        })

    return features

def create_spatial_join(housing_df, crime_df, radius_miles=0.5, sample_size=None):
    """
    Create spatial join between housing and crime data.

    Parameters:
    - housing_df: Cleaned housing dataframe (clean1)
    - crime_df: Cleaned crime dataframe (data2)
    - radius_miles: Search radius (default 0.5 miles)
    - sample_size: If specified, only process this many properties (for testing)

    Returns:
    - DataFrame with crime features (PROPERTY_ID as foreign key)
    """
    # Sample if requested (useful for testing)
    if sample_size:
        housing_sample = housing_df.sample(n=min(sample_size, len(housing_df)), random_state=42)
    else:
        housing_sample = housing_df

    print(f"Processing {len(housing_sample)} properties with {len(crime_df)} crime records...")
    print(f"Search radius: {radius_miles} miles (~{radius_miles*20:.0f} blocks)")

    # Calculate crime features for each property
    crime_features_list = []

    for idx, prop in housing_sample.iterrows():
        if idx % 100 == 0:
            print(f"Processing property {idx}/{len(housing_sample)}")

        features = aggregate_crime_features(prop, crime_df, radius_miles)
        crime_features_list.append(features)

    # Convert to dataframe
    crime_features_df = pd.DataFrame(crime_features_list)

    return crime_features_df

def create_database_schema(housing_df, crime_df, crime_features_df, db_path='nyc_real_estate_crime.db'):
    """
    Create a normalized database with proper foreign key relationships.

    Tables:
    1. properties - all property details
    2. crimes - all individual crime records
    3. property_crime_stats - aggregated crime statistics per property
    4. borough_stats - summary statistics by borough
    """
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # Enable foreign key constraints
    cursor.execute("PRAGMA foreign_keys = ON")

    print("Creating database tables...")

    # ====== TABLE 1: Properties ======
    # Primary table with all property information
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS properties (
            PROPERTY_ID INTEGER PRIMARY KEY,
            TYPE TEXT,
            PRICE INTEGER,
            BEDS INTEGER,
            BATH INTEGER,
            PROPERTYSQFT INTEGER,
            BOROUGH TEXT,
            LATITUDE REAL,
            LONGITUDE REAL,
            ZIP_CODE TEXT
        )
    """)

    housing_df.to_sql('properties', conn, if_exists='replace', index=False)
    print(f"âœ“ Created 'properties' table with {len(housing_df)} records")

    # ====== TABLE 2: Crimes ======
    # All individual crime records
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS crimes (
            CRIME_ID INTEGER PRIMARY KEY AUTOINCREMENT,
            ARREST_DATE TEXT,
            LAW_CODE TEXT,
            LAW_CAT_CD TEXT,
            LAW_CATEGORY TEXT,
            ARREST_BORO TEXT,
            BOROUGH TEXT,
            ARREST_PRECINCT INTEGER,
            AGE_GROUP TEXT,
            PERP_RACE TEXT,
            X_COORD_CD REAL,
            Y_COORD_CD REAL,
            Latitude REAL,
            Longitude REAL
        )
    """)

    # Add CRIME_ID if not present
    crime_df_with_id = crime_df.copy()
    if 'CRIME_ID' not in crime_df_with_id.columns:
        crime_df_with_id.insert(0, 'CRIME_ID', range(1, len(crime_df_with_id) + 1))

    crime_df_with_id.to_sql('crimes', conn, if_exists='replace', index=False)
    print(f"âœ“ Created 'crimes' table with {len(crime_df_with_id)} records")

    # ====== TABLE 3: Property Crime Statistics ======
    # Aggregated crime stats linked to properties via PROPERTY_ID
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS property_crime_stats (
            PROPERTY_ID INTEGER PRIMARY KEY,
            total_crimes INTEGER,
            crime_density REAL,
            felony_count INTEGER,
            misdemeanor_count INTEGER,
            violation_count INTEGER,
            felony_pct REAL,
            misdemeanor_pct REAL,
            violation_pct REAL,
            crimes_last_90_days INTEGER,
            avg_crime_distance REAL,
            min_crime_distance REAL,
            FOREIGN KEY (PROPERTY_ID) REFERENCES properties(PROPERTY_ID)
        )
    """)

    crime_features_df.to_sql('property_crime_stats', conn, if_exists='replace', index=False)
    print(f"âœ“ Created 'property_crime_stats' table with {len(crime_features_df)} records")

    # ====== TABLE 4: Borough Statistics ======
    # Summary statistics by borough
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS borough_stats (
            BOROUGH TEXT PRIMARY KEY,
            total_properties INTEGER,
            avg_price REAL,
            median_price REAL,
            total_crimes INTEGER,
            avg_crimes_per_property REAL,
            avg_crime_density REAL,
            avg_felony_pct REAL
        )
    """)

    # Calculate borough statistics
    borough_stats = []
    for borough in housing_df['BOROUGH'].unique():
        borough_props = housing_df[housing_df['BOROUGH'] == borough]
        borough_crime_stats = crime_features_df[
            crime_features_df['PROPERTY_ID'].isin(borough_props['PROPERTY_ID'])
        ]

        stats = {
            'BOROUGH': borough,
            'total_properties': len(borough_props),
            'avg_price': borough_props['PRICE'].mean(),
            'median_price': borough_props['PRICE'].median(),
            'total_crimes': borough_crime_stats['total_crimes'].sum(),
            'avg_crimes_per_property': borough_crime_stats['total_crimes'].mean(),
            'avg_crime_density': borough_crime_stats['crime_density'].mean(),
            'avg_felony_pct': borough_crime_stats['felony_pct'].mean()
        }
        borough_stats.append(stats)

    borough_stats_df = pd.DataFrame(borough_stats)
    borough_stats_df.to_sql('borough_stats', conn, if_exists='replace', index=False)
    print(f"âœ“ Created 'borough_stats' table with {len(borough_stats_df)} records")

    # Create indexes for better query performance
    print("\nCreating indexes...")
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_properties_borough ON properties(BOROUGH)")
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_properties_price ON properties(PRICE)")
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_crimes_borough ON crimes(BOROUGH)")
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_crimes_date ON crimes(ARREST_DATE)")
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_crimes_category ON crimes(LAW_CATEGORY)")
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_crime_stats_density ON property_crime_stats(crime_density)")
    print("âœ“ Created indexes")

    conn.commit()

    # Display schema information
    print("\n" + "="*60)
    print("DATABASE SCHEMA SUMMARY")
    print("="*60)

    cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
    tables = cursor.fetchall()

    for table in tables:
        table_name = table[0]
        cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
        count = cursor.fetchone()[0]
        print(f"\n{table_name}: {count} records")

        cursor.execute(f"PRAGMA table_info({table_name})")
        columns = cursor.fetchall()
        for col in columns[:5]:  # Show first 5 columns
            print(f"  - {col[1]} ({col[2]})")
        if len(columns) > 5:
            print(f"  ... and {len(columns)-5} more columns")

    print("\n" + "="*60)

    conn.close()
    print(f"\nâœ“ Database saved to: {db_path}")
    return db_path

# Example queries to demonstrate the schema
def example_queries(db_path='nyc_real_estate_crime.db'):
    """
    Show example SQL queries using the normalized schema.
    """
    conn = sqlite3.connect(db_path)

    print("\n" + "="*60)
    print("EXAMPLE QUERIES")
    print("="*60)

    # Query 1: Get properties with their crime stats
    print("\n1. Properties with crime statistics (using JOIN):")
    query1 = """
        SELECT
            p.PROPERTY_ID,
            p.PRICE,
            p.BEDS,
            p.BATH,
            p.BOROUGH,
            c.total_crimes,
            c.crime_density,
            c.felony_count
        FROM properties p
        LEFT JOIN property_crime_stats c ON p.PROPERTY_ID = c.PROPERTY_ID
        LIMIT 5
    """
    result1 = pd.read_sql(query1, conn)
    print(result1.to_string())

    # Query 2: Borough summary
    print("\n2. Borough statistics:")
    query2 = """
        SELECT
            BOROUGH,
            total_properties,
            ROUND(avg_price, 2) as avg_price,
            ROUND(avg_crimes_per_property, 1) as avg_crimes,
            ROUND(avg_felony_pct, 1) as avg_felony_pct
        FROM borough_stats
        ORDER BY avg_price DESC
    """
    result2 = pd.read_sql(query2, conn)
    print(result2.to_string())

    # Query 3: High crime vs low price properties
    print("\n3. Properties with high crime but relatively low price:")
    query3 = """
        SELECT
            p.PROPERTY_ID,
            p.PRICE,
            p.BOROUGH,
            p.BEDS,
            c.total_crimes,
            c.felony_count
        FROM properties p
        JOIN property_crime_stats c ON p.PROPERTY_ID = c.PROPERTY_ID
        WHERE c.total_crimes > 1000
        AND p.PRICE < 500000
        ORDER BY c.total_crimes DESC
        LIMIT 5
    """
    result3 = pd.read_sql(query3, conn)
    print(result3.to_string())

    # Query 4: Top 10 most expensive properties with crime density
    print("\n4. Top 10 most expensive properties with crime density:")
    query4 = """
        SELECT
            p.PROPERTY_ID,
            p.PRICE,
            p.BOROUGH,
            c.crime_density
        FROM properties p
        JOIN property_crime_stats c ON p.PROPERTY_ID = c.PROPERTY_ID
        ORDER BY p.PRICE DESC
        LIMIT 10
    """
    print(pd.read_sql(query4, conn).to_string())

    # Query 5: Boroughs ranked by crime density
    print("\n5. Boroughs ranked by average crime density:")
    query5 = """
        SELECT
            BOROUGH,
            COUNT(*) AS num_properties,
            AVG(crime_density) AS avg_crime_density
        FROM property_crime_stats pcs
        JOIN properties p ON pcs.PROPERTY_ID = p.PROPERTY_ID
        GROUP BY BOROUGH
        ORDER BY avg_crime_density DESC
    """
    print(pd.read_sql(query5, conn).to_string())

    # Query 6: Properties with zero nearby crimes
    print("\n6. Properties with zero nearby crimes:")
    query6 = """
        SELECT
            p.PROPERTY_ID,
            p.BOROUGH,
            p.PRICE
        FROM properties p
        JOIN property_crime_stats c ON p.PROPERTY_ID = c.PROPERTY_ID
        WHERE c.total_crimes = 0
        LIMIT 10
    """
    print(pd.read_sql(query6, conn).to_string())

    # Query 7: Average price by crime category dominance
    print("\n7. Average price grouped by dominant crime type:")
    query7 = """
        SELECT
            CASE
                WHEN felony_pct > misdemeanor_pct AND felony_pct > violation_pct THEN 'Felony Dominant'
                WHEN misdemeanor_pct > felony_pct AND misdemeanor_pct > violation_pct THEN 'Misdemeanor Dominant'
                ELSE 'Violation Dominant'
            END AS dominant_crime_type,
            AVG(p.PRICE) AS avg_price
        FROM properties p
        JOIN property_crime_stats c ON p.PROPERTY_ID = c.PROPERTY_ID
        GROUP BY dominant_crime_type
    """
    print(pd.read_sql(query7, conn).to_string())

    # Query 8: Crime activity trend proxy by properties
    print("\n8. Properties with highest recent crime activity:")
    query8 = """
        SELECT
            p.PROPERTY_ID,
            p.BOROUGH,
            c.crimes_last_90_days
        FROM properties p
        JOIN property_crime_stats c ON p.PROPERTY_ID = c.PROPERTY_ID
        ORDER BY c.crimes_last_90_days DESC
        LIMIT 10
    """
    print(pd.read_sql(query8, conn).to_string())

    # Query 9: Avg beds and baths by borough
    print("\n9. Average beds and baths by borough:")
    query9 = """
        SELECT
            BOROUGH,
            AVG(BEDS) AS avg_beds,
            AVG(BATH) AS avg_bath
        FROM properties
        GROUP BY BOROUGH
    """
    print(pd.read_sql(query9, conn).to_string())

    conn.close()

# Main execution
if __name__ == "__main__":
    # Assuming clean1 (housing) and data2 (crime) are loaded

    # TESTING MODE: Use sample_size parameter
    SAMPLE_SIZE = None  # Change to None to process full dataset

    # Step 1: Create crime features (spatial join)
    print("="*60)
    print("STEP 1: Creating spatial join")
    print("="*60)

    if SAMPLE_SIZE:
        print(f"ðŸ§ª TESTING MODE: Processing sample of {SAMPLE_SIZE} properties")
        print(f"   (Set SAMPLE_SIZE = None to process all {len(clean1)} properties)")

    crime_features = create_spatial_join(clean1, data2, radius_miles=0.5, sample_size=SAMPLE_SIZE)

    # For testing, also use sampled housing data
    if SAMPLE_SIZE:
        housing_sample = clean1[clean1['PROPERTY_ID'].isin(crime_features['PROPERTY_ID'])]
        print(f"\nâœ“ Sampled {len(housing_sample)} properties for database creation")
    else:
        housing_sample = clean1

    # Step 2: Create normalized database
    print("\n" + "="*60)
    print("STEP 2: Creating normalized database schema")
    print("="*60)
    db_path = create_database_schema(housing_sample, data2, crime_features,
                                      db_path='nyc_real_estate_crime_SAMPLE.db' if SAMPLE_SIZE else 'nyc_real_estate_crime.db')

    # Step 3: Show example queries
    example_queries(db_path)

    print("\nâœ“ Database creation complete!")
    print(f"\nYou now have 4 normalized tables:")
    print("  1. properties - all property details")
    print("  2. crimes - all crime records")
    print("  3. property_crime_stats - aggregated crime stats (linked via PROPERTY_ID)")
    print("  4. borough_stats - borough-level summaries")

    if SAMPLE_SIZE:
        print(f"\nâš ï¸  This is a SAMPLE database with {SAMPLE_SIZE} properties")
        print(f"   To process full dataset: Set SAMPLE_SIZE = None and re-run")

"""# Prediction Modelling"""

# Load saved database ----
import sqlite3
import pandas as pd

# Connect to database
conn = sqlite3.connect('/content/nyc_real_estate_crime.db')

# List all tables in the database
tables = pd.read_sql_query("SELECT name FROM sqlite_master WHERE type = 'table';", conn)
print("Tables found in nyc_real_estate_crime.db:")
print(tables)

# Load required data into dataframes by joining 'properties' and 'property_crime_stats'
train = pd.read_sql_query("""
    SELECT
        p.*,
        pcs.total_crimes,
        pcs.crime_density,
        pcs.felony_count,
        pcs.misdemeanor_count,
        pcs.violation_count,
        pcs.felony_pct,
        pcs.misdemeanor_pct,
        pcs.violation_pct,
        pcs.crimes_last_90_days,
        pcs.avg_crime_distance,
        pcs.min_crime_distance
    FROM properties p
    JOIN property_crime_stats pcs ON p.PROPERTY_ID = pcs.PROPERTY_ID;
""", conn)
print(train.columns)

conn.close

# Model preprocessing ----
import numpy as np

# Perform variable transformations as needed
print(f"Skewness of the Price column is: {round(train['PRICE'].skew(),3)}")
# Skewness is extremely high, indicating it is right-skewed and requires a log-transformation

train['LOG_PRICE'] = np.log1p(train['PRICE'])
print(f"Skewness of the log(Price) column is: {round(train['LOG_PRICE'].skew(),3)}")
# Skewness is now closer to zero with log-transformation

# One-hot Encoding
one_hot = pd.get_dummies(
    train[['TYPE','BOROUGH']],
    dtype = int,
    drop_first = True) # To eliminate redundancy
train_one_hot = (pd.concat([train, one_hot], axis = 1)).drop(['TYPE','BOROUGH'], axis = 1)
train_one_hot.head(5)

# Build prediction model using XGBoost ----
from sklearn.model_selection import train_test_split
from sklearn.model_selection import RandomizedSearchCV
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Define response variable and predictor variables
Y = train_one_hot['LOG_PRICE']
X = train_one_hot.drop(columns = [ # Drop unnecessary features for the model
    'PRICE',
    'LOG_PRICE',
    'PROPERTY_ID',
    'LATITUDE',
    'LONGITUDE',
    'ZIP_CODE'
], axis = 1)

# Create training and test data
X_train, X_test, Y_train, Y_test = train_test_split(
    X, Y, test_size = 0.2, random_state = 42
)

# Use randomized search to define hyperparameters
param_dist = {
    'max_depth': [3, 4, 5, 6, 7, 8],
    'learning_rate': np.linspace(0.01, 0.3, 30),
    'n_estimators': [100, 200, 300, 500, 800],
    'subsample': np.linspace(0.5, 1.0, 6),
    'colsample_bytree': np.linspace(0.5,1.0, 6),
}

xgbmod = XGBRegressor()

random_search = RandomizedSearchCV(
    estimator = xgbmod,
    param_distributions = param_dist,
    n_iter = 30,
    scoring = 'neg_mean_squared_error',
    cv = 3,
    verbose = 1,
    n_jobs = -1,
    random_state = 42
)

random_search.fit(X,Y)
print("Best parameters found: ", random_search.best_params_)

# Use best hyperparameters to train the XGBoost model
xgbmod = XGBRegressor(
    n_estimators = 500,
    learning_rate = 0.04,
    max_depth = 6,
    subsample = 0.7,
    colsample_bytree = 0.8
)
xgbmod.fit(X_train, Y_train)
print("Model training complete.")

# Save the trained model
import pickle
with open('xgboost_model.pkl', 'wb') as f:
    pickle.dump(xgbmod, f)

print("Model saved successfully!")

# Download the files
from google.colab import files
files.download('xgboost_model.pkl')
files.download('nyc_real_estate_crime.db')
# â†‘â†‘â†‘ END OF NEW CODE

# Test XGBoost model (your existing code continues...)
from sklearn.metrics import mean_absolute_error
Y_pred = xgbmod.predict(X_test)



# Test XGBoost model
from sklearn.metrics import mean_absolute_error
Y_pred = xgbmod.predict(X_test)

print("Performance metrics on response variable log(Price)")
rmse_log = np.sqrt(mean_squared_error(Y_test, Y_pred))
print("RMSE: ", round(rmse_log,2))

mae_log = mean_absolute_error(Y_test, Y_pred)
print("MAE:", round(mae_log,2))

r2_log = r2_score(Y_test, Y_pred)
print("Rsquared: ", round(r2_log,2))

"""Final trained model has an Rsquared = 0.80, which meets our target of Rsquared > 0.70."""

import shap
import matplotlib.pyplot as plt

explainer = shap.TreeExplainer(xgbmod)

# Calculate SHAP values for the test set
shap_values = explainer.shap_values(X_test)
shap.summary_plot(shap_values, X_test)

# Get the base value (average log price prediction)
base_value = explainer.expected_value

# Visualize the explanation for the first test instance
shap.force_plot(
    base_value,
    shap_values[0, :],
    X_test.iloc[0, :],
    matplotlib=True,
    show=False
)
plt.title("SHAP Force Plot for a Single Property")
plt.show()

"""SHAP values demonstrate that property-specific attributes contribute the most to price (bath, bed, square footage).

The severity of crimes seem to weigh more in pricing, as percentage counts of crimes (felonies, violation, misdemeanor) contribute more than total counts in an area. The occurence of more recent crimes also weighs higher.

Plots made to visualize the dataset
"""

import matplotlib.pyplot as plt
import sqlite3
import pandas as pd

conn = sqlite3.connect('/content/nyc_real_estate_crime.db')

# Load joined data for plotting
plot_df = pd.read_sql("""
    SELECT
        p.PRICE, p.BOROUGH, p.BEDS, p.BATH,
        c.total_crimes, c.crime_density, c.crimes_last_90_days
    FROM properties p
    JOIN property_crime_stats c ON p.PROPERTY_ID = c.PROPERTY_ID
""", conn)

conn.close()

# Plot 1: Price vs Total Crimes
plt.figure()
plt.scatter(plot_df["total_crimes"], plot_df["PRICE"])
plt.title("Price vs Total Crimes")
plt.xlabel("Total Crimes Nearby")
plt.ylabel("Property Price")
plt.show()

# Plot 2: Price vs Crime Density
plt.figure()
plt.scatter(plot_df["crime_density"], plot_df["PRICE"])
plt.title("Price vs Crime Density")
plt.xlabel("Crime Density")
plt.ylabel("Property Price")
plt.show()

# Plot 3: Avg Price by Borough
borough_price = plot_df.groupby("BOROUGH")["PRICE"].mean()
plt.figure()
borough_price.plot(kind="bar")
plt.title("Average Property Price by Borough")
plt.xlabel("Borough")
plt.ylabel("Average Price")
plt.show()

# Plot 4: Total Crimes by Borough
borough_crime = plot_df.groupby("BOROUGH")["total_crimes"].mean()
plt.figure()
borough_crime.plot(kind="bar")
plt.title("Average Crimes per Property by Borough")
plt.xlabel("Borough")
plt.ylabel("Avg Crimes")
plt.show()

# Plot 5: Beds vs Price
plt.figure()
plt.scatter(plot_df["BEDS"], plot_df["PRICE"])
plt.title("Beds vs Price")
plt.xlabel("Number of Beds")
plt.ylabel("Price")
plt.show()

# Plot 6: Crimes in last 90 days vs Price
plt.figure()
plt.scatter(plot_df["crimes_last_90_days"], plot_df["PRICE"])
plt.title("Recent Crimes vs Price")
plt.xlabel("Crimes in Last 90 Days")
plt.ylabel("Price")
plt.show()

"""Predictive Model without crime features"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor
from sklearn.metrics import r2_score, mean_squared_error

# 1. Load data
data1 = pd.read_csv('/content/NY-House-Dataset.csv')

# 2. Replicate NY House Dataset cleaning to get 'clean1'
clean1 = data1.copy()

# Drop duplicates
clean1.drop_duplicates(inplace=True)

# Standardize numeric data and handle specific outliers
clean1['BATH'] = clean1['BATH'].astype(int)
clean1['PROPERTYSQFT'] = clean1['PROPERTYSQFT'].astype(int)
# Drop outliers manually identified in the notebook's cleaning step
clean1.drop([4691, 622], inplace=True)

# Drop unnecessary columns
clean1.drop(['BROKERTITLE','LONG_NAME','STREET_NAME','ADMINISTRATIVE_AREA_LEVEL_2','ADDRESS','MAIN_ADDRESS','FORMATTED_ADDRESS','LOCALITY'], axis=1, inplace=True)

# Standardize text labels: Map SUBLOCALITY to BOROUGH
borough_mapping = {
    'New York County':'Manhattan', 'Kings County':'Brooklyn', 'Queens County':'Queens',
    'Bronx County':'The Bronx', 'Richmond County':'Staten Island', 'East Bronx':'The Bronx',
    'Riverdale':'The Bronx', 'Coney Island':'Brooklyn', 'Brooklyn Heights':'Brooklyn',
    'Fort Hamilton':'Brooklyn', 'Dumbo':'Brooklyn', 'Snyder Avenue':'Brooklyn',
    'Jackson Heights':'Queens', 'Rego Park':'Queens', 'Flushing':'Queens',
    'New York':'Manhattan'
}
clean1['SUBLOCALITY'] = clean1['SUBLOCALITY'].replace(borough_mapping)
clean1.rename(columns={'SUBLOCALITY':'BOROUGH'}, inplace=True)

# Extract and drop ZIP_CODE
clean1['ZIP_CODE'] = clean1['STATE'].str.extract(r'(\d{5})')
clean1.drop(['STATE'], axis=1, inplace=True)

# Clean property TYPE labels
clean1['TYPE'] = clean1['TYPE'].str.split(' ').str[0]
type_mapping = {
    'Multi-family':'Multi-family home', 'For':'For sale', 'Coming':'Coming soon',
    'Mobile':'Mobile house', 'Condop':'Condo'
}
clean1['TYPE'] = clean1['TYPE'].replace(type_mapping)

# Add PROPERTY_ID (used for consistency but will be dropped in X later)
clean1['PROPERTY_ID'] = range(1, len(clean1) + 1)

# The 'clean1' dataframe now represents the housing data without any crime features.

# 3. Model Preprocessing (Log-transform and One-Hot Encoding)
train_no_crime = clean1.copy()
train_no_crime['LOG_PRICE'] = np.log1p(train_no_crime['PRICE'])

# One-hot Encoding
one_hot = pd.get_dummies(
    train_no_crime[['TYPE', 'BOROUGH']],
    dtype=int,
    drop_first=True
)
train_one_hot_no_crime = (pd.concat([train_no_crime, one_hot], axis=1)).drop(['TYPE', 'BOROUGH'], axis=1)

# 4. Define Response and Predictor Variables
Y_no_crime = train_one_hot_no_crime['LOG_PRICE']
X_no_crime = train_one_hot_no_crime.drop(columns=[
    'PRICE', # Original target (not used)
    'LOG_PRICE', # Transformed target (Y)
    'PROPERTY_ID', # Identifier (not used)
    'LATITUDE', # Geographic data (dropped for model simplicity/consistency)
    'LONGITUDE', # Geographic data (dropped for model simplicity/consistency)
    'ZIP_CODE' # Granular location data (dropped for model simplicity/consistency)
], axis=1)

# 5. Create training and test data
X_train_nc, X_test_nc, Y_train_nc, Y_test_nc = train_test_split(
    X_no_crime, Y_no_crime, test_size=0.2, random_state=42
)

# 6. Train XGBoost Model (using best hyperparameters from the original notebook's search)
# Best parameters found: {'subsample': 0.7, 'n_estimators': 500, 'max_depth': 6, 'learning_rate': 0.04, 'colsample_bytree': 0.8}
xgbmod_no_crime = XGBRegressor(
    n_estimators=500,
    learning_rate=0.04,
    max_depth=6,
    subsample=0.7,
    colsample_bytree=0.8,
    random_state=42,
    n_jobs=-1
)
xgbmod_no_crime.fit(X_train_nc, Y_train_nc)

# 7. Predict and Calculate R-squared
Y_pred_no_crime = xgbmod_no_crime.predict(X_test_nc)
r2_no_crime = r2_score(Y_test_nc, Y_pred_no_crime)

print(f"R-squared value for the price prediction model (excluding crime data): {round(r2_no_crime, 4)}")